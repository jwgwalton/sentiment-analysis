{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "This notebook is a practice of sentiment analysis using Keras on the US airline twitter dataset available at https://www.kaggle.com/crowdflower/twitter-airline-sentiment/downloads/twitter-airline-sentiment.zip/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 15 columns):\n",
      "tweet_id                        14640 non-null int64\n",
      "airline_sentiment               14640 non-null object\n",
      "airline_sentiment_confidence    14640 non-null float64\n",
      "negativereason                  9178 non-null object\n",
      "negativereason_confidence       10522 non-null float64\n",
      "airline                         14640 non-null object\n",
      "airline_sentiment_gold          40 non-null object\n",
      "name                            14640 non-null object\n",
      "negativereason_gold             32 non-null object\n",
      "retweet_count                   14640 non-null int64\n",
      "text                            14640 non-null object\n",
      "tweet_coord                     1019 non-null object\n",
      "tweet_created                   14640 non-null object\n",
      "tweet_location                  9907 non-null object\n",
      "user_timezone                   9820 non-null object\n",
      "dtypes: float64(2), int64(2), object(11)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to only look at the text data for analysing the sentiment and will restrict myself to classification of positive/neutral/negative\n",
    "\n",
    "columns_to_keep = [\n",
    "    \"airline_sentiment\",\n",
    "    \"text\"\n",
    "]\n",
    "\n",
    "df = df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 2 columns):\n",
      "airline_sentiment    14640 non-null object\n",
      "text                 14640 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 228.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    9178\n",
       "neutral     3099\n",
       "positive    2363\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"airline_sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# None of the rows contain nulls, however the classes of the target variable are unevenly distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and encoding\n",
    "\n",
    "1) The target variable needs to be encoded as an int\n",
    "\n",
    "2) The text needs to be cleaned so it is lower case and only contains alpha-numerica characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: Emoji's contain interesting information and are emotional qualifiers\n",
    "\n",
    "Sentiment for \"Ryanair your customer service is great *laughing face* is completely wrong if the emoji is removed\n",
    "\n",
    "Possible approaches\n",
    "\n",
    "1) Lookup table for word equivalent\n",
    "\n",
    "2) emoji2vec https://github.com/uclmr/emoji2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target variable encoding to int\n",
    "sentiments = {\n",
    "    \"negative\": 0,\n",
    "    \"neutral\" : 1,\n",
    "    \"positive\": 2,\n",
    "}\n",
    "\n",
    "df[\"sentiment\"] = df[\"airline_sentiment\"].apply(lambda x: sentiments[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def clean_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return \" \".join([t for t in tokens if t.isalpha()]).strip().replace(\"\\n\", \" \").lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cleaned_text\"] = df[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     virginamerica what dhepburn said\n",
       "1    virginamerica plus you added commercials to th...\n",
       "2    virginamerica i did today must mean i need to ...\n",
       "3    virginamerica it really aggressive to blast ob...\n",
       "4    virginamerica and it a really big bad thing ab...\n",
       "Name: cleaned_text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"cleaned_text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the average number of words in the tweets?\n",
    "\n",
    "This will be useful in deciding the length of the sequences for the LSTM in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"number_of_words\"] = df[\"cleaned_text\"].apply(lambda x: len(x.split(\" \")))\n",
    "                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa33e222160>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"number_of_words\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximum_words_in_a_tweet = df[\"number_of_words\"].max()\n",
    "maximum_words_in_a_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"cleaned_text\"].values\n",
    "y = df[\"sentiment\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word vector embeddings\n",
    "\n",
    "1) Create a lookup table for the tokens and their vectors using a fasttext model, included padding and unknown (out-of-vocabulary) tokens and vectors for them\n",
    "\n",
    "2) Tokenize the text into a list of tokens\n",
    "\n",
    "3) pass the lookup table to the embedding layer in the model so it can perform the lookup for the vector for each token,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "\n",
    "en_model = fasttext.load_model(\"../fasttext/cc.en.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = en_model.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras uses index 0 for padding, so first token in words will be looked up in the matrix, actual token doesn't matter\n",
    "# specifiying the unknown token so then when they're tokenized it uses it and looks up the correct token in the weights matrix\n",
    "padding_token = \"<pad>\"\n",
    "unknown_token = \"<unk>\"\n",
    "\n",
    "words = [padding_token, unknown_token] + en_model.get_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_index = {word: index for index, word in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup table for keras embedding layer\n",
    "\n",
    "weights = np.zeros((len(word_2_index), embedding_dimension))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000002, 300)\n"
     ]
    }
   ],
   "source": [
    "# Populate the matrix with the vectors from fasttext, if unknown tokens put random vector, if padding set to 0-vector.\n",
    "\n",
    "for word, i in word_2_index.items():\n",
    "    if word == \"<pad>\":\n",
    "        vec = np.zeros(embedding_dimension) # vector of zeros for padding\n",
    "    elif word == \"<unk>\":\n",
    "        vec = np.random.uniform(low=-1.0, high=1.0, size=(embedding_dimension,)) # random vector for unknown words\n",
    "\n",
    "    #elif word in en_model.get_words():\n",
    "    #    vec = en_model.get_word_vector(word)\n",
    "    #else:\n",
    "    #    # word not in fasttext corpus, return vector for unknown token\n",
    "    #    vec = np.random.uniform(low=-1.0, high=1.0, size=(embedding_dimension,)) # random vector for unknown words\n",
    "    \n",
    "    else:\n",
    "        vec = en_model.get_word_vector(word)\n",
    "    weights[i] = vec\n",
    "    \n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=unknown_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10256"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many words were found in the tokenizer? These would be a lot less than the total vocabulary for the fasttext model\n",
    "\n",
    "# I did use this for the weights matrix but it is relying on the train set containing all the tokens we would see in the \n",
    "# test set/ any later incoming data\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virginamerica what dhepburn said\n",
      "[[81, 49, 5249, 209]]\n"
     ]
    }
   ],
   "source": [
    "print(df[\"cleaned_text\"].values[0])\n",
    "print(tokenizer.texts_to_sequences([df[\"cleaned_text\"].values[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target variable encoding\n",
    "\n",
    "Need to encode to categorical variable for multi class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_target_classes = len(df[\"sentiment\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y_cat_train = to_categorical(y_train, num_classes=number_of_target_classes)\n",
    "y_cat_test = to_categorical(y_test, num_classes=number_of_target_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with class imbalance\n",
    "\n",
    "add class weights for the classifier to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53173522, 1.57482856, 2.06451613])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=np.argmax(y_cat_train, axis=1))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding for the model\n",
    "\n",
    "Tweets have a maximum character length of 140 so the maxlength of the sequences going into the model can be of length 140 with post padding to extend any tweets shorter than the maximum length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_length = maximum_words_in_a_tweet # use the max tweet size we've seen in the data to minimize padding\n",
    "\n",
    "X_train_pad = pad_sequences(X_train, max_length, padding=\"post\")\n",
    "X_test_pad = pad_sequences(X_test, max_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11712, 32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras import layers\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimensions = 300 # number of dimensions for the word vector embeddings\n",
    "number_of_classes = 3 # positive/neutral/negative\n",
    "dropout_ratio = 0.3 # % of neurons to drop out to stop overfitting\n",
    "vocabulary_size = len(words) # the number of words found in the train set which have been encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session() # Memory issues if don't restart session\n",
    "\n",
    "inputs = layers.Input(shape=(max_length,))\n",
    "\n",
    "embedding = layers.Embedding(input_dim=vocabulary_size, \n",
    "                      output_dim=embedding_dimensions, \n",
    "                      input_length=max_length,\n",
    "                      weights=[weights],\n",
    "                      trainable=False, # Don't want it overfitting, went to use this as a lookup to the fasttext word vectors\n",
    "                     )(inputs)\n",
    "\n",
    "x = layers.SpatialDropout1D(dropout_ratio)(embedding)\n",
    "\n",
    "x = layers.Conv1D(32, 4, padding=\"same\")(x)\n",
    "\n",
    "x = layers.MaxPooling1D(padding=\"same\")(x)\n",
    "x = layers.Activation(\"tanh\")(x)\n",
    "x = layers.Dropout(dropout_ratio)(x)\n",
    "\n",
    "x = layers.Conv1D(16, 2, padding=\"same\")(x) \n",
    "\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(dropout_ratio)(x)\n",
    "\n",
    "#x = layers.MaxPooling1D(padding=\"same\")(x)\n",
    "#x = layers.Dropout(dropout_ratio)(x)\n",
    "#x = layers.Flatten()(x)\n",
    "\n",
    "outputs = layers.Dense(number_of_classes, activation=\"softmax\")(x)\n",
    "model = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "model.compile(RMSprop(lr=0.005),\n",
    "             \"categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 32, 300)           600000600 \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 32, 300)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 32, 32)            38432     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 16, 32)            0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16, 16)            1040      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 600,040,123\n",
      "Trainable params: 39,523\n",
      "Non-trainable params: 600,000,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping_harsh = EarlyStopping(monitor=\"val_loss\",patience=1) # stops overfitting very well\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\",patience=2)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=1, min_lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9369 samples, validate on 2343 samples\n",
      "Epoch 1/20\n",
      "9369/9369 [==============================] - 4s 378us/step - loss: 0.7983 - acc: 0.6735 - val_loss: 0.7316 - val_acc: 0.7136\n",
      "Epoch 2/20\n",
      "9369/9369 [==============================] - 3s 332us/step - loss: 0.7342 - acc: 0.7070 - val_loss: 0.7237 - val_acc: 0.7068\n",
      "Epoch 3/20\n",
      "9369/9369 [==============================] - 3s 332us/step - loss: 0.7019 - acc: 0.7141 - val_loss: 0.6789 - val_acc: 0.7273\n",
      "Epoch 4/20\n",
      "9369/9369 [==============================] - 3s 338us/step - loss: 0.6866 - acc: 0.7237 - val_loss: 0.6999 - val_acc: 0.7204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9fc465f048>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_pad, \n",
    "          y_cat_train,\n",
    "          batch_size=16,\n",
    "          class_weight=class_weights, # use class weights to deal with imbalanced classes\n",
    "          epochs=20,\n",
    "          #callbacks=[early_stopping, reduce_lr],\n",
    "          callbacks=[early_stopping_harsh],\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.746484  , 0.18773806, 0.06577791], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1722,   70,   44],\n",
       "       [ 383,  175,   62],\n",
       "       [ 200,   57,  215]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_cat_test.argmax(axis=1), y_pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7213114754098361"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_cat_test.argmax(axis=1), y_pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict    0          1          2          \n",
      "Actual\n",
      "0          1722       70         44         \n",
      "\n",
      "1          383        175        62         \n",
      "\n",
      "2          200        57         215        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Overall Statistics : \n",
      "\n",
      "95% CI                                                            (0.70507,0.73755)\n",
      "ACC Macro                                                         0.81421\n",
      "AUNP                                                              0.68397\n",
      "AUNU                                                              0.67393\n",
      "Bennett S                                                         0.58197\n",
      "CBA                                                               0.49495\n",
      "Chi-Squared                                                       1062.79498\n",
      "Chi-Squared DF                                                    4\n",
      "Conditional Entropy                                               0.74556\n",
      "Cramer V                                                          0.42601\n",
      "Cross Entropy                                                     1.42451\n",
      "F1 Macro                                                          0.58451\n",
      "F1 Micro                                                          0.72131\n",
      "Gwet AC1                                                          0.63881\n",
      "Hamming Loss                                                      0.27869\n",
      "Joint Entropy                                                     2.06646\n",
      "KL Divergence                                                     0.1036\n",
      "Kappa                                                             0.40305\n",
      "Kappa 95% CI                                                      (0.36827,0.43784)\n",
      "Kappa No Prevalence                                               0.44262\n",
      "Kappa Standard Error                                              0.01775\n",
      "Kappa Unbiased                                                    0.38995\n",
      "Lambda A                                                          0.25275\n",
      "Lambda B                                                          0.02408\n",
      "Mutual Information                                                0.21382\n",
      "NIR                                                               0.62705\n",
      "Overall ACC                                                       0.72131\n",
      "Overall CEN                                                       0.43818\n",
      "Overall J                                                         (1.31811,0.43937)\n",
      "Overall MCC                                                       0.4298\n",
      "Overall MCEN                                                      0.56785\n",
      "Overall RACC                                                      0.53314\n",
      "Overall RACCU                                                     0.54317\n",
      "P-Value                                                           None\n",
      "PPV Macro                                                         0.66544\n",
      "PPV Micro                                                         0.72131\n",
      "Pearson C                                                         0.51605\n",
      "Phi-Squared                                                       0.36298\n",
      "RCI                                                               0.16188\n",
      "RR                                                                976.0\n",
      "Reference Entropy                                                 1.3209\n",
      "Response Entropy                                                  0.95938\n",
      "SOA1(Landis & Koch)                                               Moderate\n",
      "SOA2(Fleiss)                                                      Intermediate to Good\n",
      "SOA3(Altman)                                                      Moderate\n",
      "SOA4(Cicchetti)                                                   Fair\n",
      "SOA5(Cramer)                                                      Relatively Strong\n",
      "SOA6(Matthews)                                                    Weak\n",
      "Scott PI                                                          0.38995\n",
      "Standard Error                                                    0.00829\n",
      "TPR Macro                                                         0.55856\n",
      "TPR Micro                                                         0.72131\n",
      "Zero-one Loss                                                     816\n",
      "\n",
      "Class Statistics :\n",
      "\n",
      "Classes                                                           0             1             2             \n",
      "ACC(Accuracy)                                                     0.76195       0.80464       0.87602       \n",
      "AGF(Adjusted F-score)                                             0.79601       0.5174        0.66619       \n",
      "AGM(Adjusted geometric mean)                                      0.6082        0.70534       0.79551       \n",
      "AM(Difference between automatic and manual classification)        469           -318          -151          \n",
      "AUC(Area under the ROC curve)                                     0.70201       0.61362       0.70617       \n",
      "AUCI(AUC value interpretation)                                    Good          Fair          Good          \n",
      "AUPR(Area under the PR curve)                                     0.84249       0.43086       0.56265       \n",
      "BCD(Bray-Curtis dissimilarity)                                    0.08009       0.0543        0.02579       \n",
      "BM(Informedness or bookmaker informedness)                        0.40403       0.22723       0.41235       \n",
      "CEN(Confusion entropy)                                            0.34899       0.65951       0.64659       \n",
      "DOR(Diagnostic odds ratio)                                        13.18796      6.75352       18.54673      \n",
      "DP(Discriminant power)                                            0.61759       0.45734       0.69923       \n",
      "DPI(Discriminant power interpretation)                            Poor          Poor          Poor          \n",
      "ERR(Error rate)                                                   0.23805       0.19536       0.12398       \n",
      "F0.5(F0.5 score)                                                  0.77876       0.47867       0.61219       \n",
      "F1(F1 score - harmonic mean of precision and sensitivity)         0.83168       0.37961       0.54224       \n",
      "F2(F2 score)                                                      0.89232       0.31452       0.48665       \n",
      "FDR(False discovery rate)                                         0.25293       0.42053       0.33022       \n",
      "FN(False negative/miss/type 2 error)                              114           445           257           \n",
      "FNR(Miss rate or false negative rate)                             0.06209       0.71774       0.54449       \n",
      "FOR(False omission rate)                                          0.18299       0.16946       0.09858       \n",
      "FP(False positive/type 1 error/false alarm)                       583           127           106           \n",
      "FPR(Fall-out or false positive rate)                              0.53388       0.05503       0.04316       \n",
      "G(G-measure geometric mean of precision and sensitivity)          0.83707       0.40443       0.55235       \n",
      "GI(Gini index)                                                    0.40403       0.22723       0.41235       \n",
      "GM(G-mean geometric mean of specificity and sensitivity)          0.66119       0.51646       0.66019       \n",
      "IBA(Index of balanced accuracy)                                   0.64343       0.08996       0.21734       \n",
      "IS(Information score)                                             0.25267       1.45238       2.05482       \n",
      "J(Jaccard index)                                                  0.71186       0.23427       0.37197       \n",
      "LS(Lift score)                                                    1.19141       2.73659       4.15492       \n",
      "MCC(Matthews correlation coefficient)                             0.47739       0.30523       0.48532       \n",
      "MCCI(Matthews correlation coefficient interpretation)             Weak          Weak          Weak          \n",
      "MCEN(Modified confusion entropy)                                  0.48569       0.69775       0.74384       \n",
      "MK(Markedness)                                                    0.56409       0.41001       0.5712        \n",
      "N(Condition negative)                                             1092          2308          2456          \n",
      "NLR(Negative likelihood ratio)                                    0.13321       0.75954       0.56905       \n",
      "NLRI(Negative likelihood ratio interpretation)                    Fair          Negligible    Negligible    \n",
      "NPV(Negative predictive value)                                    0.81701       0.83054       0.90142       \n",
      "OC(Overlap coefficient)                                           0.93791       0.57947       0.66978       \n",
      "OOC(Otsuka-Ochiai coefficient)                                    0.83707       0.40443       0.55235       \n",
      "OP(Optimized precision)                                           0.42593       0.26464       0.52106       \n",
      "P(Condition positive or support)                                  1836          620           472           \n",
      "PLR(Positive likelihood ratio)                                    1.75677       5.12954       10.55405      \n",
      "PLRI(Positive likelihood ratio interpretation)                    Poor          Fair          Good          \n",
      "POP(Population)                                                   2928          2928          2928          \n",
      "PPV(Precision or positive predictive value)                       0.74707       0.57947       0.66978       \n",
      "PRE(Prevalence)                                                   0.62705       0.21175       0.1612        \n",
      "Q(Yule Q - coefficient of colligation)                            0.85904       0.74205       0.89768       \n",
      "RACC(Random accuracy)                                             0.49363       0.02184       0.01767       \n",
      "RACCU(Random accuracy unbiased)                                   0.50004       0.02479       0.01834       \n",
      "TN(True negative/correct rejection)                               509           2181          2350          \n",
      "TNR(Specificity or true negative rate)                            0.46612       0.94497       0.95684       \n",
      "TON(Test outcome negative)                                        623           2626          2607          \n",
      "TOP(Test outcome positive)                                        2305          302           321           \n",
      "TP(True positive/hit)                                             1722          175           215           \n",
      "TPR(Sensitivity, recall, hit rate, or true positive rate)         0.93791       0.28226       0.45551       \n",
      "Y(Youden index)                                                   0.40403       0.22723       0.41235       \n",
      "dInd(Distance index)                                              0.53748       0.71985       0.5462        \n",
      "sInd(Similarity index)                                            0.61994       0.49099       0.61378       \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pycm import ConfusionMatrix\n",
    "matrix = ConfusionMatrix(y_cat_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
