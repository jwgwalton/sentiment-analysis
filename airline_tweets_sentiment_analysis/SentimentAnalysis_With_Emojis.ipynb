{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "This notebook is a practice of sentiment analysis using Keras on the US airline twitter dataset available at https://www.kaggle.com/crowdflower/twitter-airline-sentiment/downloads/twitter-airline-sentiment.zip/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 15 columns):\n",
      "tweet_id                        14640 non-null int64\n",
      "airline_sentiment               14640 non-null object\n",
      "airline_sentiment_confidence    14640 non-null float64\n",
      "negativereason                  9178 non-null object\n",
      "negativereason_confidence       10522 non-null float64\n",
      "airline                         14640 non-null object\n",
      "airline_sentiment_gold          40 non-null object\n",
      "name                            14640 non-null object\n",
      "negativereason_gold             32 non-null object\n",
      "retweet_count                   14640 non-null int64\n",
      "text                            14640 non-null object\n",
      "tweet_coord                     1019 non-null object\n",
      "tweet_created                   14640 non-null object\n",
      "tweet_location                  9907 non-null object\n",
      "user_timezone                   9820 non-null object\n",
      "dtypes: float64(2), int64(2), object(11)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to only look at the text data for analysing the sentiment and will restrict myself to classification of positive/neutral/negative\n",
    "\n",
    "columns_to_keep = [\n",
    "    \"airline_sentiment\",\n",
    "    \"text\"\n",
    "]\n",
    "\n",
    "df = df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 2 columns):\n",
      "airline_sentiment    14640 non-null object\n",
      "text                 14640 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 228.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    9178\n",
       "neutral     3099\n",
       "positive    2363\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"airline_sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# None of the rows contain nulls, however the classes of the target variable are unevenly distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and encoding\n",
    "\n",
    "1) The target variable needs to be encoded as an int\n",
    "\n",
    "2) The text needs to be cleaned so it is lower case and only contains alpha-numerica characters\n",
    "\n",
    "## NOTE: Emoji's contain interesting information and are emotional qualifiers\n",
    "\n",
    "Sentiment for \"Ryanair your customer service is great *laughing face* is completely wrong if the emoji is removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target variable encoding to int\n",
    "sentiments = {\n",
    "    \"negative\": 0,\n",
    "    \"neutral\" : 1,\n",
    "    \"positive\": 2,\n",
    "}\n",
    "\n",
    "df[\"sentiment\"] = df[\"airline_sentiment\"].apply(lambda x: sentiments[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import string\n",
    "#lookup_table = str.maketrans({key: None for key in string.punctuation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def clean_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return \" \".join([t for t in tokens if t.isalpha()]).strip().replace(\"\\n\", \" \").lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cleaned_text\"] = df[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     virginamerica what dhepburn said\n",
       "1    virginamerica plus you added commercials to th...\n",
       "2    virginamerica i did today must mean i need to ...\n",
       "3    virginamerica it really aggressive to blast ob...\n",
       "4    virginamerica and it a really big bad thing ab...\n",
       "Name: cleaned_text, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"cleaned_text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the average number of words in the tweets?\n",
    "\n",
    "This will be useful in deciding the length of the sequences for the LSTM in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"number_of_words\"] = df[\"cleaned_text\"].apply(lambda x: len(x.split(\" \")))\n",
    "                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb839af3208>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"number_of_words\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximum_words_in_a_tweet = df[\"number_of_words\"].max()\n",
    "maximum_words_in_a_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How often do the tweets have emoji's in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "def extract_emojis(str):\n",
    "    return ''.join(c for c in str if c in emoji.UNICODE_EMOJI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"emojis\"] = df[\"text\"].apply(extract_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         14147\n",
       "😊           18\n",
       "👍           16\n",
       "😒           15\n",
       "😉           14\n",
       "         ...  \n",
       "💩            1\n",
       "😅            1\n",
       "😩😂😂😂😂        1\n",
       "❄❄❄          1\n",
       "😍😍😍          1\n",
       "Name: emojis, Length: 236, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"emojis\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# There aren't actually that many emojis so it wouldn't give much information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"cleaned_text\"].values\n",
    "y = df[\"sentiment\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TODO: do i need to stratify to maintain the distribution of the target variable.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, ,stratify=y, test_size=0.2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word vector embeddings\n",
    "\n",
    "1) Create a lookup table for the tokens and their vectors using a fasttext model, included padding and unknown (out-of-vocabulary) tokens and vectors for them\n",
    "\n",
    "2) Tokenize the text into a list of tokens\n",
    "\n",
    "3) pass the lookup table to the embedding layer in the model so it can perform the lookup for the vector for each token,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "\n",
    "en_model = fasttext.load_model(\"../fasttext/cc.en.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = en_model.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras uses index 0 for padding, so first token in words will be looked up in the matrix, actual token doesn't matter\n",
    "# specifiying the unknown token so then when they're tokenized it uses it and looks up the correct token in the weights matrix\n",
    "padding_token = \"<pad>\"\n",
    "unknown_token = \"<unk>\"\n",
    "\n",
    "words = [padding_token, unknown_token] + en_model.get_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_index = {word: index for index, word in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup table for keras embedding layer\n",
    "\n",
    "weights = np.zeros((len(word_2_index), embedding_dimension))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000002, 300)\n"
     ]
    }
   ],
   "source": [
    "# Populate the matrix with the vectors from fasttext, if unknown tokens put random vector, if padding set to 0-vector.\n",
    "\n",
    "for word, i in word_2_index.items():\n",
    "    if word == \"<pad>\":\n",
    "        vec = np.zeros(embedding_dimension) # vector of zeros for padding\n",
    "    elif word == \"<unk>\":\n",
    "        vec = np.random.uniform(low=-1.0, high=1.0, size=(embedding_dimension,)) # random vector for unknown words\n",
    "\n",
    "    #elif word in en_model.get_words():\n",
    "    #    vec = en_model.get_word_vector(word)\n",
    "    #else:\n",
    "    #    # word not in fasttext corpus, return vector for unknown token\n",
    "    #    vec = np.random.uniform(low=-1.0, high=1.0, size=(embedding_dimension,)) # random vector for unknown words\n",
    "    \n",
    "    else:\n",
    "        vec = en_model.get_word_vector(word)\n",
    "    weights[i] = vec\n",
    "    \n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=unknown_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10208"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many words were found in the tokenizer? These would be a lot less than the total vocabulary for the fasttext model\n",
    "\n",
    "# I did use this for the weights matrix but it is relying on the train set containing all the tokens we would see in the \n",
    "# test set/ any later incoming data\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virginamerica what dhepburn said\n",
      "[[76, 49, 9291, 222]]\n"
     ]
    }
   ],
   "source": [
    "print(df[\"cleaned_text\"].values[0])\n",
    "print(tokenizer.texts_to_sequences([df[\"cleaned_text\"].values[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target variable encoding\n",
    "\n",
    "Need to encode to categorical variable for multi class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_target_classes = len(df[\"sentiment\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y_cat_train = to_categorical(y_train, num_classes=number_of_target_classes)\n",
    "y_cat_test = to_categorical(y_test, num_classes=number_of_target_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with class imbalance\n",
    "\n",
    "add class weights for the classifier to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53464804, 1.56285028, 2.041841  ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=np.argmax(y_cat_train, axis=1))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding for the model\n",
    "\n",
    "Tweets have a maximum character length of 140 so the maxlength of the sequences going into the model can be of length 140 with post padding to extend any tweets shorter than the maximum length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_length = maximum_words_in_a_tweet # use the max tweet size we've seen in the data to minimize padding\n",
    "\n",
    "X_train_pad = pad_sequences(X_train, max_length, padding=\"post\")\n",
    "X_test_pad = pad_sequences(X_test, max_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11712, 32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras import layers\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimensions = 300 # number of dimensions for the word vector embeddings\n",
    "number_of_classes = 3 # positive/neutral/negative\n",
    "dropout_ratio = 0.2 # % of neurons to drop out to stop overfitting\n",
    "vocabulary_size = len(words) # the number of words found in the train set which have been encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session() # Memory issues if don't restart session\n",
    "\n",
    "inputs = layers.Input(shape=(max_length,))\n",
    "\n",
    "embedding = layers.Embedding(input_dim=vocabulary_size, \n",
    "                      output_dim=embedding_dimensions, \n",
    "                      input_length=max_length,\n",
    "                      weights=[weights],\n",
    "                      trainable=False, # Don't want it overfitting, went to use this as a lookup to the fasttext word vectors\n",
    "                     )(inputs)\n",
    "\n",
    "# 32 cells as this is the maximum word length of a tweet\n",
    "# Bidirectional so 64\n",
    "\n",
    "lstm = layers.LSTM(maximum_words_in_a_tweet, dropout=dropout_ratio, recurrent_dropout=dropout_ratio, return_sequences=True)\n",
    "x = layers.Bidirectional(lstm)(embedding)\n",
    "\n",
    "# worked as a way to flatten the data\n",
    "#x = layers.LSTM(10)(x)\n",
    "\n",
    "# i'm returning the sequences so i can use the convolutions after this but would it be better to reshape the vector to have a single channel\n",
    "#lstm = layers.LSTM(maximum_words_in_a_tweet, dropout=dropout_ratio, recurrent_dropout=dropout_ratio, return_sequences=True)\n",
    "#x = layers.Bidirectional(lstm)(embedding)\n",
    "#x = layers.Reshape((maximum_words_in_a_tweet*2, 1))(x)\n",
    "\n",
    "x = layers.Dropout(dropout_ratio)(x)\n",
    "\n",
    "x = layers.Conv1D(16, 5, padding=\"same\")(x) # 5 strides to check for large scale strcture\n",
    "x = layers.MaxPooling1D(padding=\"same\")(x)\n",
    "x = layers.Activation(\"tanh\")(x)\n",
    "x = layers.Dropout(dropout_ratio)(x)\n",
    "\n",
    "x = layers.SpatialDropout1D(dropout_ratio)(x)\n",
    "\n",
    "x = layers.Conv1D(16, 2, padding=\"same\")(x) # 2 strides to check for fine structure \n",
    "x = layers.MaxPooling1D(padding=\"same\")(x)\n",
    "x = layers.Activation(\"tanh\")(x)\n",
    "x = layers.Dropout(dropout_ratio)(x)\n",
    "\n",
    "x = layers.SpatialDropout1D(dropout_ratio)(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "x = layers.Dropout(dropout_ratio)(x)\n",
    "outputs = layers.Dense(number_of_classes, activation=\"softmax\")(x)\n",
    "model = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11712, 3)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.predict(X_train_pad)\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "model.compile(RMSprop(lr=0.001),\n",
    "             \"categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 32, 300)           600000600 \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 32, 64)            85248     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 32, 16)            5136      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 16, 16)            0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16, 16)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16)            0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 16, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16, 16)            528       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 8, 16)             0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 8, 16)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 16)             0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 8, 16)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 600,095,739\n",
      "Trainable params: 95,139\n",
      "Non-trainable params: 600,000,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\",patience=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9369 samples, validate on 2343 samples\n",
      "Epoch 1/20\n",
      "9369/9369 [==============================] - 76s 8ms/step - loss: 0.8266 - acc: 0.6554 - val_loss: 0.7292 - val_acc: 0.6987\n",
      "Epoch 2/20\n",
      "9369/9369 [==============================] - 74s 8ms/step - loss: 0.7541 - acc: 0.6980 - val_loss: 0.6958 - val_acc: 0.7226\n",
      "Epoch 3/20\n",
      "9369/9369 [==============================] - 75s 8ms/step - loss: 0.7259 - acc: 0.7132 - val_loss: 0.6733 - val_acc: 0.7243\n",
      "Epoch 4/20\n",
      "9369/9369 [==============================] - 75s 8ms/step - loss: 0.6989 - acc: 0.7213 - val_loss: 0.6687 - val_acc: 0.7281\n",
      "Epoch 5/20\n",
      "9369/9369 [==============================] - 76s 8ms/step - loss: 0.6868 - acc: 0.7287 - val_loss: 0.6774 - val_acc: 0.7341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdb8f983748>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_pad, \n",
    "          y_cat_train,\n",
    "          batch_size=16,\n",
    "          class_weight=class_weights, # use class weights to deal with imbalanced classes\n",
    "          epochs=20,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5771884 , 0.38622266, 0.03658898], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1821,   37,   18],\n",
       "       [ 440,  145,   16],\n",
       "       [ 272,   32,  147]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_cat_test.argmax(axis=1), y_pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7216530054644809"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_cat_test.argmax(axis=1), y_pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'pycm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-98fd51eed595>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpycm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfusionMatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfusionMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_cat_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'pycm'"
     ]
    }
   ],
   "source": [
    "from pycm import ConfusionMatrix\n",
    "matrix = ConfusionMatrix(y_cat_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
